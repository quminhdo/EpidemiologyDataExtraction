{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6342,
     "status": "ok",
     "timestamp": 1620796496453,
     "user": {
      "displayName": "18020717 Trần Văn Khoa",
      "photoUrl": "",
      "userId": "14104662109094842134"
     },
     "user_tz": -420
    },
    "id": "ruH3tc6c_7Hx",
    "outputId": "e5e33974-153e-4442-fcde-34b1ecffbac7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from shutil import copyfile\n",
    "import os\n",
    "\n",
    "csv_dir = 'csv'\n",
    "label_file = '20210513_Labeling.csv'\n",
    "df = pd.read_csv(label_file, error_bad_lines=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "l4V0096gABTn"
   },
   "outputs": [],
   "source": [
    "def getpos(para, asw):\n",
    "    if len(str(asw))>len(str(para)): return -1\n",
    "    l = len(str(asw))\n",
    "    thr=20\n",
    "    if l<thr: return str(para).lower().find(str(asw).lower())\n",
    "    for i in range(l,l//2,-1):\n",
    "        for j in range(0,l-i+1):\n",
    "            out = str(para).lower().find(str(asw[j:j+i]).lower())\n",
    "            if out!=-1: return out-j\n",
    "    return -1\n",
    "\n",
    "def ppstr(s):\n",
    "    for i in range(5,1,-1):\n",
    "        prev = s[0]\n",
    "        while prev!=s:\n",
    "            prev = s\n",
    "            s = s.replace(' '*i, ' ')\n",
    "    return s\n",
    "def pplabel(s):\n",
    "    char = ''' `~1234567890-=!@#$%^&*()_+qwertyuiopasdfghjklzxcvbnmQWERTYUIOPASDFGHJKLZXCVBNM<>?\":{}|][';/.,\\ '''\n",
    "    for i in s:\n",
    "        if i not in char:\n",
    "            s = s.replace(i, '**')\n",
    "    try: return ppstr(s)\n",
    "    except: return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "erUNzx8vAHCR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[0, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[0, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[0, 1, 1, 1]\n",
      "[0, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[0, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[0, 1, 0, 0]\n",
      "[0, 1, 1, 1]\n",
      "[1, 1, 1, 0]\n",
      "[0, 1, 1, 1]\n",
      "[0, 1, 1, 0]\n",
      "[0, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[0, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[0, 1, 1, 0]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[0, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 0]\n",
      "[1, 1, 0, 0]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 0, 0]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 0]\n",
      "[1, 1, 1, 0]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 0]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 0, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 0]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 0]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 0]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 0]\n",
      "[1, 1, 1, 0]\n",
      "[0, 1, 1, 0]\n",
      "[1, 1, 1, 0]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 0, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 0]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 0]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 0]\n",
      "[1, 1, 1, 1]\n",
      "[0, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[0, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[0, 1, 1, 0]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 0, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 0]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 0, 1]\n",
      "[1, 1, 0, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 0]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 0, 1]\n",
      "[1, 1, 1, 0]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 0]\n",
      "[0, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 0]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 0]\n",
      "[1, 1, 1, 1]\n",
      "[134 153 144 126]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "fields = ['First Author', 'Publication Year', 'Study Title', 'Study Description']\n",
    "full_df = []\n",
    "lack_df = []\n",
    "stat = np.array([0 for i in fields])\n",
    "list_isfull = []\n",
    "count = 0\n",
    "for i, row in df.iterrows():\n",
    "    assert len(list_isfull)==i\n",
    "    try:\n",
    "        df_pdf = pd.read_csv('{}/{}'.format(csv_dir, row['File Name'].replace('txt', 'csv')))\n",
    "        df_pdf['str']\n",
    "    except Exception as e:\n",
    "        count+=1\n",
    "        list_isfull.append('')\n",
    "        print(count, e)\n",
    "        continue\n",
    "    isfull = [0 for i in fields]\n",
    "    out = {'data':[]}\n",
    "    for j, block in df_pdf.iterrows():\n",
    "        para = block['str'][2:-2].replace(r'\\n', ' ').replace('\\n', ' ').replace('- ', '').replace('?','**')\n",
    "        try: para = ppstr(para)\n",
    "        except: pass\n",
    "        for idx_field, field in enumerate(fields):\n",
    "            d = {}\n",
    "            d['context'] = para\n",
    "            d['question'] = field\n",
    "            d['id'] = \"{0}_{1}_{2}\".format(i,j,field)\n",
    "            d['title'] = row['File Name']\n",
    "            d['answers'] = {'answer_start': [], 'text': []}\n",
    "            label = pplabel(str(row[field]))\n",
    "            if True:\n",
    "                pos = getpos(para, label)\n",
    "                if pos!=-1:\n",
    "                    d['answers']['answer_start'].append(pos)\n",
    "                    d['answers']['text'].append(str(row[field]))\n",
    "                    isfull[idx_field] = 1\n",
    "            out['data'].append(d)\n",
    "    if sum(isfull)==len(fields):\n",
    "        with open(os.path.join('fulllabel',row['File Name'].replace('txt', 'json')), 'w') as f:\n",
    "            json.dump(out, f)\n",
    "        full_df.append(i)\n",
    "    else:\n",
    "        with open(os.path.join('lacklabel',row['File Name'].replace('txt', 'json')), 'w') as f:\n",
    "            json.dump(out, f)\n",
    "        lack_df.append(i)\n",
    "    print(isfull)\n",
    "    stat += np.array(isfull)\n",
    "    list_isfull.append('{0}_{1}_{2}_{3}'.format(isfull[0],isfull[1],isfull[2],isfull[3]))\n",
    "    # break\n",
    "print(stat)\n",
    "df['available'] = list_isfull\n",
    "df.to_csv('csv.csv')\n",
    "df.iloc[full_df].to_csv(os.path.join('fulllabel', '0.full.csv'))\n",
    "df.iloc[lack_df].to_csv(os.path.join('lacklabel', '0.lack.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOBJEzGgHaIXmUTZub3b7m6",
   "collapsed_sections": [],
   "mount_file_id": "1ZaVUvzcRNa8WCigT5PmkAkbrzV74B7ko",
   "name": "convert_df2json.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
